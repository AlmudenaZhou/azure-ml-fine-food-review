{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f0fa278",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11f7c24",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931b5408",
   "metadata": {},
   "source": [
    "The objective of the model is to determine whether a review conveys positive or negative sentiment. The data processing steps focus on cleaning the data to ensure optimal input for the model.\n",
    "\n",
    "### Data Preparation:\n",
    "- Only the text and Score fields are retained.\n",
    "- For sentiment classification:\n",
    "    - Scores ≤ 2 are considered negative.\n",
    "    - Scores ≥ 4 are considered positive.\n",
    "    - Scores of 3 are excluded.\n",
    "- Text cleaning simplifies the vectorized representation of the data.\n",
    "\n",
    "### Cleaning Pipelines:\n",
    "Two dynamic pipelines are used for text cleaning to facilitate experimentation:\n",
    "\n",
    "1. Sentence-Based Cleaning (using a class structure).\n",
    "2. Token-Based Cleaning (using a function-based pipeline).\n",
    "\n",
    "#### Sentence-Based Cleaning Steps:\n",
    "- RemoveNumbers: Eliminates words containing numbers and standalone numbers.\n",
    "- RemoveHtml: Strips out HTML tags.\n",
    "- RemoveUrl: Removes URLs starting with \"http\" or \"www\".\n",
    "- RemovePatterns: Deletes words with three or more identical letters (e.g., aaaaa, zzzzzz, or heeeelloooo).\n",
    "- RemoveAbbreviations: Expands abbreviations to their full forms (e.g., haven't → have not).\n",
    "\n",
    "#### Token-Based Cleaning Steps:\n",
    "- remove_stopwords: Excludes English stopwords while retaining abbreviated word forms.\n",
    "- stem_text: Applies stemming to reduce words to their root forms.\n",
    "- lemmatize_text: Performs lemmatization for grammatical consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "393da04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = pd.read_csv('../data/cleaned_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aea071f",
   "metadata": {},
   "source": [
    "## Binary labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d3988f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_binary(cleaned_data):\n",
    "    df_binary = pd.DataFrame(cleaned_data, columns=['Score', 'Text'])\n",
    "    #create a binary dataset with only the positive or negative reviews\n",
    "\n",
    "    # Create a function to map scores to 0 or 1 based on your conditions\n",
    "    def label_score(score):\n",
    "        if int(score) >= 4:\n",
    "            return 1\n",
    "        elif int(score) <= 2:\n",
    "            return 0\n",
    "        else:\n",
    "            return None  # Ignore scores equal to 3\n",
    "\n",
    "    # Apply the function to the 'Score' column and create a new column 'Label'\n",
    "    df_binary['Label'] = df_binary['Score'].apply(label_score)\n",
    "\n",
    "    # Drop rows with Label equal to None (scores equal to 3)\n",
    "    df_binary = df_binary.dropna(subset=['Label'])\n",
    "\n",
    "    # Optionally, you can reset the index if you want\n",
    "    df_binary.reset_index(drop=True, inplace=True)\n",
    "    return df_binary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33cdf91",
   "metadata": {},
   "source": [
    "## Get only text and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2a3dfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_binary = data_to_binary(cleaned_data)\n",
    "\n",
    "X = df_binary.Text\n",
    "y = df_binary.Label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e62d0a",
   "metadata": {},
   "source": [
    "## Text processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbbf193",
   "metadata": {},
   "source": [
    "### Functions for tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13e0b2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\a.ramirez.lopez\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\a.ramirez.lopez\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\a.ramirez.lopez\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from spellchecker import SpellChecker\n",
    "# import string\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "def stem_text(tokens):\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    try:\n",
    "        tokens = [stemmer.stem(word) for word in tokens]\n",
    "    except TypeError:\n",
    "        print(tokens)\n",
    "    return tokens\n",
    "\n",
    "def lemmatize_text(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    default_stopwords = set(stopwords.words('english'))\n",
    "    excluding = set(['against','not','don', \"don't\",'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\",\n",
    "             'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", \n",
    "             'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\",'shouldn', \"shouldn't\", 'wasn',\n",
    "             \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"])\n",
    " \n",
    "    custom_stopwords = default_stopwords - excluding\n",
    "\n",
    "    tokens = [token for token in tokens if token not in custom_stopwords]\n",
    "    tokens = filter(None, tokens)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def correct_spelling(tokens):\n",
    "    spell = SpellChecker()\n",
    "    tokens = [spell.correction(word) for word in tokens]\n",
    "    tokens = filter(None, tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e28c7e1",
   "metadata": {},
   "source": [
    "### Sentence classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd4b3898",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b7123d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC\n",
    "\n",
    "\n",
    "class SentenceDfCleaner(ABC):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.pattern: str\n",
    "\n",
    "    def clean(self, df):\n",
    "        return df.str.replace(self.pattern, '', regex=True)\n",
    "\n",
    "\n",
    "class RemoveNumbers(SentenceDfCleaner):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.pattern = re.compile(\"\\S*\\d\\S*\")\n",
    "\n",
    "\n",
    "class RemoveHtml(SentenceDfCleaner):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.pattern = re.compile('<.*?>')\n",
    "\n",
    "\n",
    "class RemoveUrl(SentenceDfCleaner):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.pattern = re.compile('http\\S+|www.\\S+')\n",
    "\n",
    "\n",
    "class RemovePunctuations(SentenceDfCleaner):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.pattern = re.compile('[^\\w\\s]')\n",
    "\n",
    "\n",
    "class RemovePatterns(SentenceDfCleaner):\n",
    "    \"\"\"\n",
    "    https://stackoverflow.com/questions/37012948/regex-to-match-an-entire-word-that-contains-repeated-character\n",
    "    Remove words like 'zzzzzzzzzzzzzzzzzzzzzzz', 'testtting', 'grrrrrrreeeettttt' etc. \n",
    "    Preserves words like 'looks', 'goods', 'soon' etc. We will remove all such words \n",
    "    which has three consecutive repeating characters.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.pattern = re.compile('\\\\s*\\\\b(?=\\\\w*(\\\\w)\\\\1{2,})\\\\w*\\\\b')\n",
    "\n",
    "\n",
    "class RemoveAbbreviations(SentenceDfCleaner):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.abbr_dict = {\n",
    "            \"what's\":\"what is\",\n",
    "            \"what're\":\"what are\",\n",
    "            \"who's\":\"who is\",\n",
    "            \"who're\":\"who are\",\n",
    "            \"where's\":\"where is\",\n",
    "            \"where're\":\"where are\",\n",
    "            \"when's\":\"when is\",\n",
    "            \"when're\":\"when are\",\n",
    "            \"how's\":\"how is\",\n",
    "            \"how're\":\"how are\",\n",
    "\n",
    "            \"i'm\":\"i am\",\n",
    "            \"we're\":\"we are\",\n",
    "            \"you're\":\"you are\",\n",
    "            \"they're\":\"they are\",\n",
    "            \"it's\":\"it is\",\n",
    "            \"he's\":\"he is\",\n",
    "            \"she's\":\"she is\",\n",
    "            \"that's\":\"that is\",\n",
    "            \"there's\":\"there is\",\n",
    "            \"there're\":\"there are\",\n",
    "\n",
    "            \"i've\":\"i have\",\n",
    "            \"we've\":\"we have\",\n",
    "            \"you've\":\"you have\",\n",
    "            \"they've\":\"they have\",\n",
    "            \"who've\":\"who have\",\n",
    "            \"would've\":\"would have\",\n",
    "            \"not've\":\"not have\",\n",
    "\n",
    "            \"i'll\":\"i will\",\n",
    "            \"we'll\":\"we will\",\n",
    "            \"you'll\":\"you will\",\n",
    "            \"he'll\":\"he will\",\n",
    "            \"she'll\":\"she will\",\n",
    "            \"it'll\":\"it will\",\n",
    "            \"they'll\":\"they will\",\n",
    "\n",
    "            \"isn't\":\"is not\",\n",
    "            \"wasn't\":\"was not\",\n",
    "            \"aren't\":\"are not\",\n",
    "            \"weren't\":\"were not\",\n",
    "            \"can't\":\"can not\",\n",
    "            \"couldn't\":\"could not\",\n",
    "            \"don't\":\"do not\",\n",
    "            \"didn't\":\"did not\",\n",
    "            \"shouldn't\":\"should not\",\n",
    "            \"wouldn't\":\"would not\",\n",
    "            \"doesn't\":\"does not\",\n",
    "            \"haven't\":\"have not\",\n",
    "            \"hasn't\":\"has not\",\n",
    "            \"hadn't\":\"had not\",\n",
    "            \"won't\":\"will not\",\n",
    "            '\\s+':' '\n",
    "        }\n",
    "        self.pattern = re.compile(\"|\".join(map(re.escape, self.abbr_dict.keys())))\n",
    "    \n",
    "    def clean(self, df):\n",
    "        return df.str.replace(self.pattern, \n",
    "                              lambda match: self.abbr_dict[match.group(0)],\n",
    "                                regex=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd460d7",
   "metadata": {},
   "source": [
    "### Pipeline functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bee412dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ind_preprocess_text(text, processing_steps, tokenized=False):\n",
    "    ''' Put everything in lowercase, remove punctuation and stopwords --> possibility to do stemming or lemmatizaion'''\n",
    "    # Tokenize the text and convert to lowercase every word\n",
    "    if not isinstance(text, list):\n",
    "        tokens = word_tokenize(text)\n",
    "    else:\n",
    "        tokens = text\n",
    "    \n",
    "    for processing_step in processing_steps:\n",
    "        tokens = processing_step(tokens)\n",
    "    \n",
    "    if tokenized:\n",
    "        return tokens\n",
    "    # Join tokens back into a single string\n",
    "    return TreebankWordDetokenizer().detokenize(tokens)\n",
    "\n",
    "\n",
    "def preprocess_text(text_df, processing_steps, tokenized):\n",
    "    text_df = text_df.str.lower()\n",
    "\n",
    "    for sent_step in processing_steps['sentence']:\n",
    "        text_df = sent_step.clean(text_df)\n",
    "    \n",
    "    text_df = text_df.apply(ind_preprocess_text, \n",
    "                         processing_steps=processing_steps['tokens'], \n",
    "                         tokenized=tokenized)\n",
    "    return text_df\n",
    "\n",
    "\n",
    "processing_steps = {'sentence': [RemoveNumbers(), RemoveHtml(), RemoveUrl(), RemovePunctuations(), \n",
    "                                 RemovePatterns(), RemoveAbbreviations()],\n",
    "                    'tokens': [remove_stopwords, stem_text, lemmatize_text]}\n",
    "\n",
    "# Example usage:\n",
    "X_processed = preprocess_text(X, processing_steps, tokenized=False)\n",
    "# X_processed2 = preprocess_text(X_processed, processing_steps, tokenized=False, is_clean=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54277086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bought sever vital can dog food product found ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>product arriv label jumbo salt peanutsth peanu...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>confect around centuri light pillowi citrus ge...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>great taffi great price wide assort yummi taff...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>got wild hair taffi order five pound bag taffi...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Labels\n",
       "0  bought sever vital can dog food product found ...     1.0\n",
       "1  product arriv label jumbo salt peanutsth peanu...     0.0\n",
       "2  confect around centuri light pillowi citrus ge...     1.0\n",
       "3  great taffi great price wide assort yummi taff...     1.0\n",
       "4  got wild hair taffi order five pound bag taffi...     1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_processed = pd.concat([X_processed, y], axis=1)\n",
    "data_processed.columns = ['Text', 'Labels']\n",
    "data_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "818f9e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processed.to_csv('../data/processed_text_with_all.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
